{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import math\n",
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating vocabulary list\n",
    "#cleaning data\n",
    "#storing in variables \n",
    "tweets_pos= list()\n",
    "tweets_neg= list()\n",
    "tweets_neu= list()\n",
    "vocab_pos = list()\n",
    "count_pos = list()\n",
    "vocab_neg = list()\n",
    "count_neg = list()\n",
    "vocab_neu = list()\n",
    "count_neu = list()\n",
    "XTE_pos = list()\n",
    "XTE_neg = list()\n",
    "XTE_neu = list()\n",
    "XTR_pos = list()\n",
    "XTR_neg = list()\n",
    "XTR_neu = list()\n",
    "T_vocab = list()\n",
    "\n",
    "m=0 #number of data examples/tweets\n",
    "stop_words = ['a', 'an', 'the', 'it','i','to','in','my','am','is','if','for','be','and','im' ,'you','herself', 'it', 'its', 'been', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'of', 'at', 'by', 'for', 'with', 'to', 'from']\n",
    "exclude= ['-','@','#','',':',';','...','.','..','!','/','']\n",
    "tweet = 0\n",
    "\n",
    "with open('/Users/syedhadi/Desktop/Assignment4/Tweets.csv') as file:\n",
    "    iterator = csv.reader(file, delimiter=',')\n",
    "    for text in iterator:\n",
    "        if tweet == 0:\n",
    "            tweet += 1\n",
    "        else:\n",
    "            st1 = text[1].lower()\n",
    "            st1=st1.replace('@',' ')\n",
    "            st1=st1.replace('#','')\n",
    "            st1=st1.replace('$','')\n",
    "            st1=st1.replace(':',' ')\n",
    "            st1=st1.replace(';',' ')\n",
    "            st1=st1.replace('/','')\n",
    "            st1=st1.replace('.',' ')\n",
    "            st1=st1.replace('&',' ')\n",
    "            st1=st1.replace('-',' ')\n",
    "            st1=st1.replace('=','')\n",
    "            st1=st1.replace('!','')\n",
    "            st1=st1.replace('?','')\n",
    "            st1=st1.replace(',','')\n",
    "            st1=st1.replace('\"','')\n",
    "            st1=st1.replace('~',' ')\n",
    "            st1=st1.replace('(','')\n",
    "            st1=st1.replace(')','')\n",
    "            st1=st1.replace('+','')\n",
    "            st1=st1.replace('*',' ')\n",
    "            st1=st1.replace('^',' ')\n",
    "            st1=st1.replace('“','')\n",
    "            st1=st1.replace('‘','')\n",
    "            st1=st1.replace(\"'\",\"\")\n",
    "            st1=st1.replace(\"{\",\"\")\n",
    "            st1=st1.replace(\"}\",\"\")\n",
    "            \n",
    "            if(text[0]=='positive'):\n",
    "                tweets_pos.append(st1)\n",
    "            if(text[0]=='negative'):\n",
    "                tweets_neg.append(st1)\n",
    "            if(text[0]=='neutral'):\n",
    "                tweets_neu.append(st1)            \n",
    "                    \n",
    "            tweet += 1\n",
    "            \n",
    "#Train Test Split\n",
    "\n",
    "for i in range (len(tweets_pos)):\n",
    "    if i > (int(0.8*(len(tweets_pos)))):\n",
    "        XTE_pos.append(tweets_pos[i])\n",
    "    else:\n",
    "        XTR_pos.append(tweets_pos[i])\n",
    "for i in range (len(tweets_neg)):\n",
    "    if i > (int(0.8*(len(tweets_neg)))):\n",
    "        XTE_neg.append(tweets_neg[i])\n",
    "    else:\n",
    "        XTR_neg.append(tweets_neg[i])\n",
    "for i in range (len(tweets_neu)):\n",
    "    if i > (int(0.8*(len(tweets_neu)))):\n",
    "        XTE_neu.append(tweets_neu[i])\n",
    "    else:\n",
    "        XTR_neu.append(tweets_neu[i])\n",
    "\n",
    "\n",
    "tweets_pos=None\n",
    "tweets_neg=None\n",
    "tweets_neu=None\n",
    "\n",
    "#Data cleaning and attributing\n",
    "\n",
    "#Creating Test data\n",
    "\n",
    "for eg in XTR_pos:\n",
    "    words= eg.split()\n",
    "    for i in words:\n",
    "        if i in stop_words or i.isnumeric() or i in exclude:\n",
    "            continue\n",
    "        if d.check(i):\n",
    "            if i in vocab_pos:\n",
    "                count_pos[vocab_pos.index(i)]+=1\n",
    "            else:\n",
    "                vocab_pos.append(i)\n",
    "                count_pos.append(1)\n",
    "        else:\n",
    "            if (d.suggest(i)):\n",
    "                suggested = ((d.suggest(i)[0]).lower()).split()\n",
    "                for subword in suggested:\n",
    "                    if subword in vocab_pos:\n",
    "                        count_pos[vocab_pos.index(subword)]+=1\n",
    "                    else:\n",
    "                        vocab_pos.append(subword)\n",
    "                        count_pos.append(1)\n",
    "        if i not in T_vocab:\n",
    "            T_vocab.append(i)\n",
    "                \n",
    "                \n",
    "for eg in XTR_neg:\n",
    "    words= eg.split()\n",
    "    for i in words:\n",
    "        if i in stop_words or i.isnumeric() or i in exclude:\n",
    "            continue\n",
    "        if d.check(i):\n",
    "            if i in vocab_neg:\n",
    "                count_neg[vocab_neg.index(i)]+=1\n",
    "            else:\n",
    "                vocab_neg.append(i)\n",
    "                count_neg.append(1)\n",
    "        else:\n",
    "            if (d.suggest(i)):\n",
    "                suggested = ((d.suggest(i)[0]).lower()).split()\n",
    "                for subword in suggested:\n",
    "                    if subword in vocab_neg:\n",
    "                        count_neg[vocab_neg.index(subword)]+=1\n",
    "                    else:\n",
    "                        vocab_neg.append(subword)\n",
    "                        count_neg.append(1)\n",
    "        if i not in T_vocab:\n",
    "            T_vocab.append(i)\n",
    "       \n",
    "\n",
    "for eg in XTR_neu:\n",
    "    words= eg.split()\n",
    "    for i in words:\n",
    "        if i in stop_words or i.isnumeric() or i in exclude:\n",
    "            continue\n",
    "        if d.check(i):\n",
    "            if i in vocab_neu:\n",
    "                count_neu[vocab_neu.index(i)]+=1\n",
    "            else:\n",
    "                vocab_neu.append(i)\n",
    "                count_neu.append(1)\n",
    "        else:\n",
    "            if (d.suggest(i)):\n",
    "                suggested = ((d.suggest(i)[0]).lower()).split()\n",
    "                for subword in suggested:\n",
    "                    if subword in vocab_neu:\n",
    "                        count_neu[vocab_neu.index(subword)]+=1\n",
    "                    else:\n",
    "                        vocab_neu.append(subword)\n",
    "                        count_neu.append(1)\n",
    "        if i not in T_vocab:\n",
    "            T_vocab.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positive generator with Add-1 Smoothing\n",
    "def PosGen(test):\n",
    "    P_pos= len(XTR_pos)/(len(XTR_pos)+len(XTR_neg)+len(XTR_neu))\n",
    "    words= test.split()\n",
    "    for i in words:\n",
    "        if not d.check(i):\n",
    "            if (d.suggest(i)):\n",
    "                suggested = ((d.suggest(i)[0]).lower()).split()\n",
    "                for subword in suggested:\n",
    "                    if subword in T_vocab:\n",
    "                        if subword in vocab_pos:\n",
    "                            P_pos *= ((count_pos[vocab_pos.index(subword)])+1)/sum(count_pos,len(T_vocab))\n",
    "                        else:\n",
    "                            P_pos *= (1)/sum(count_pos,len(T_vocab))\n",
    "                            \n",
    "        \n",
    "        else:\n",
    "            if i in T_vocab:\n",
    "                if i in vocab_pos:\n",
    "                    P_pos *= ((count_pos[vocab_pos.index(i)])+1)/sum(count_pos,len(T_vocab))\n",
    "                else:\n",
    "                    P_pos *= (1)/sum(count_pos,len(T_vocab))\n",
    "        \n",
    "    return P_pos\n",
    "                    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Negative generator with Add-1 Smoothing\n",
    "def NegGen(test):\n",
    "    P_neg= len(XTR_neg)/(len(XTR_pos)+len(XTR_neg)+len(XTR_neu))\n",
    "    words= test.split()\n",
    "    for i in words:\n",
    "        if not d.check(i):\n",
    "            if (d.suggest(i)):\n",
    "                suggested = ((d.suggest(i)[0]).lower()).split()\n",
    "                for subword in suggested:\n",
    "                    if subword in T_vocab:\n",
    "                        if subword in vocab_neg:\n",
    "                            P_neg *= ((count_neg[vocab_neg.index(subword)])+1)/sum(count_neg,len(T_vocab))\n",
    "                        else:\n",
    "                            P_neg *= (1)/sum(count_neg,len(T_vocab))\n",
    "        \n",
    "        else:\n",
    "            if i in T_vocab:\n",
    "                if i in vocab_neg:\n",
    "                    P_neg *= (count_neg[vocab_neg.index(i)]+1)/sum(count_neg,len(T_vocab))\n",
    "                else:\n",
    "                    P_neg *= (1)/sum(count_neg,len(T_vocab))\n",
    "        \n",
    "    return P_neg\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neutral generator with Add-1 Smoothing\n",
    "def NeuGen(test):\n",
    "    P_neu= len(XTR_neu)/(len(XTR_pos)+len(XTR_neg)+len(XTR_neu))\n",
    "    words= test.split()\n",
    "    for i in words:\n",
    "        if not d.check(i):\n",
    "            if (d.suggest(i)):\n",
    "                suggested = ((d.suggest(i)[0]).lower()).split()\n",
    "                for subword in suggested:\n",
    "                    if subword in T_vocab:\n",
    "                        if subword in vocab_neu:\n",
    "                            P_neu *= ((count_neu[vocab_neu.index(subword)])+1)/sum(count_neu,len(T_vocab))\n",
    "                        else:\n",
    "                            P_neu *= (1)/sum(count_neu,len(T_vocab))\n",
    "        \n",
    "        else:\n",
    "            if i in T_vocab:\n",
    "                if i in vocab_neu:\n",
    "                    P_neu *= (count_neu[vocab_neu.index(i)]+1)/sum(count_neu,len(T_vocab))\n",
    "                else:\n",
    "                    P_neu *= (1)/sum(count_neu,len(T_vocab))\n",
    "        \n",
    "    return P_neu\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier Function, input is a raw tweet, output is the sentiment\n",
    "\n",
    "def classify(test):\n",
    "    X= np.zeros(3)\n",
    "    X[1] = PosGen(test)\n",
    "    X[2] = NegGen(test)\n",
    "    X[0] = NeuGen(test)\n",
    "    return (np.ndarray.tolist(X).index(max(X)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Macro ACCURACY CALCULATED HERE\n",
    "p=0\n",
    "ng=0\n",
    "nu=0\n",
    "for i in ((XTE_pos)):\n",
    "    if (classify(i) == 1):\n",
    "        p+=1\n",
    "for i in ((XTE_neg)):\n",
    "    if(classify(i) == 2):\n",
    "        ng+=1\n",
    "for i in ((XTE_neu)):\n",
    "    if(classify(i) == 0):\n",
    "        nu+=1\n",
    "\n",
    "Accuracy = (p+ng+nu)/(len(XTE_neg)+len(XTE_pos)+len(XTE_neu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measures Calculated here \n",
    "tp0 = 0\n",
    "fp0 = 0\n",
    "tn0 = 0\n",
    "fn0 = 0\n",
    "\n",
    "tp1 = 0\n",
    "fp1 = 0\n",
    "tn1 = 0\n",
    "fn1 = 0\n",
    "\n",
    "tp2 = 0\n",
    "fp2 = 0\n",
    "tn2 = 0\n",
    "fn2 = 0\n",
    "c=0\n",
    "\n",
    "for i in (XTE_neu):\n",
    "    pred = classify(i)\n",
    "    if (pred == 0):\n",
    "        c+=1\n",
    "        tp0+=1 ###\n",
    "        tn1+=1\n",
    "        tn2+=1\n",
    "        ##############\n",
    "    if (pred != 0):\n",
    "        fn0+=1\n",
    "        if pred==1:\n",
    "            c+=1\n",
    "            fp1+=1 \n",
    "            tn2+=1\n",
    "        if pred==2:\n",
    "            c+=1\n",
    "            fp2+=1\n",
    "            tn1+=1\n",
    "\n",
    "for i in (XTE_pos):\n",
    "    pred = classify(i)\n",
    "    if pred==1:\n",
    "        c+=1\n",
    "        tp1+=1 ###\n",
    "        tn0+=1\n",
    "        tn2+=1\n",
    "    if (pred != 1):\n",
    "        fn1+=1\n",
    "        if pred==0:\n",
    "            c+=1\n",
    "            fp0+=1 \n",
    "            tn2+=1\n",
    "        if pred==2:\n",
    "            c+=1\n",
    "            fp2+=1\n",
    "            tn0+=1\n",
    "\n",
    "for i in (XTE_neg):\n",
    "    pred = classify(i)\n",
    "    if pred==2:\n",
    "        c+=1\n",
    "        tp2+=1 ###\n",
    "        tn0+=1\n",
    "        tn1+=1\n",
    "    if (pred != 2):\n",
    "        fn2+=1\n",
    "        if pred==0:\n",
    "            c+=1\n",
    "            fp0+=1 \n",
    "            tn1+=1\n",
    "        if pred==1:\n",
    "            c+=1\n",
    "            fp1+=1\n",
    "            tn0+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Average Measures :\n",
      "Precision =  76 %\n",
      "Recall =  61 %\n",
      "Accuracy =  77 %\n",
      "F1- Measure =  65 %\n"
     ]
    }
   ],
   "source": [
    "#MACROAVERAGE MEASURES\n",
    "#Precision \n",
    "P0 = tp0/(fp0+tp0)\n",
    "P1 = tp1/(fp1+tp1)\n",
    "P2 = tp2/(fp2+tp2)\n",
    "\n",
    "#Recall\n",
    "R0 = tp0/(tp0+fn0)\n",
    "R1 = tp1/(tp1+fn1)\n",
    "R2 = tp2/(tp2+fn2)\n",
    "\n",
    "#F-1 Measure\n",
    "F1_0 = (2*R0*P0)/(R0+P0)\n",
    "F1_1 = (2*R1*P1)/(R1+P1)\n",
    "F1_2 = (2*R2*P2)/(R2+P2)\n",
    "\n",
    "F1 = (F1_0+F1_1+F1_2)/3\n",
    "\n",
    "print('Macro Average Measures :')\n",
    "print('Precision = ', math.ceil(((P0+P1+P2)/3)*100),'%')\n",
    "print('Recall = ', math.ceil(((R0+R1+R2)/3)*100),'%')\n",
    "print('Accuracy = ', math.ceil(Accuracy*100),'%')\n",
    "print('F1- Measure = ', math.ceil(F1*100),'%' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro Average Measures :\n",
      "Precision =  77 %\n",
      "Recall =  77 %\n",
      "Accuracy =  77 %\n",
      "F1- Measure =  77 %\n"
     ]
    }
   ],
   "source": [
    "#MicroAverage Measures \n",
    "tps=tp0+tp1+tp2\n",
    "fps=fp0+fp1+fp2\n",
    "fns=fn0+fn1+fn2\n",
    "\n",
    "Pr = (tps)/(tps+fps)\n",
    "Re = (tps)/(tps+fns)\n",
    "\n",
    "MicF1 = (2*Pr*Re)/(Pr+Re)\n",
    "\n",
    "print('Micro Average Measures :')\n",
    "print('Precision = ', math.ceil((Pr)*100),'%')\n",
    "print('Recall = ', math.ceil((Re)*100),'%')\n",
    "print('Accuracy = ', math.ceil((Re)*100),'%')\n",
    "print('F1- Measure = ', math.ceil(MicF1*100),'%' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
